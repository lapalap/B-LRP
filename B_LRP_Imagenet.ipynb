{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c53fgoYou-j4"
   },
   "source": [
    "# B-LRP: Imagenet experiment\n",
    "This notebook visualises the B-LRP results using the LRP-Epsilon rule and perform a pixelflipping experiment on a dolwnoaded subset of Imagenet dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHounPIedmUF"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.ticker as ticker\n",
    "import copy\n",
    "\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAknugdvvRkr"
   },
   "source": [
    "First, we import a VGG16 network, pretrained on Imagenet. We adjust the standard dropout layers in a way, that allows us to record the dropped neurons, which we need to compute LRP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "colab_type": "code",
    "id": "wR9d53ODePRj",
    "outputId": "73751b6d-6094-4738-fef0-5f75b2623f39"
   },
   "outputs": [],
   "source": [
    "class MyDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(MyDropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.seed = 0\n",
    "    \n",
    "    \n",
    "    def forward(self, input, freeze = False):\n",
    "        # if model.eval(), don't apply dropout\n",
    "        if not self.training:\n",
    "            return input\n",
    "        \n",
    "        if not freeze:\n",
    "            q=np.random.randint(10000000, size = 1)[0]\n",
    "            self.seed = q\n",
    "        \n",
    "        torch.manual_seed(self.seed)   \n",
    "        return torch.nn.functional.dropout(input, p=self.p)\n",
    "        \n",
    "        \n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            MyDropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            MyDropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg16(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n",
    "    \n",
    "\n",
    "model = vgg16()\n",
    "model.load_state_dict(torchvision.models.vgg16(pretrained=True).state_dict())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IeTcQ7ewFBN"
   },
   "source": [
    "In a following cell we define functions for LRP-CMP rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdoZMgoUgl6X"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# LRP Composite rule\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# More information at http://www.heatmapping.org/tutorial/\n",
    "\n",
    "def LRP_CMP(image, class_id, model, dropout = True, verbose = False, device = 'cpu'):\n",
    "\n",
    "\n",
    "    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(1,-1,1,1).to(device)  # We define mean and std for data normalisation\n",
    "    std  = torch.Tensor([0.229, 0.224, 0.225]).reshape(1,-1,1,1).to(device)\n",
    "\n",
    "    image = image.to(device)\n",
    "    \n",
    "    if not dropout:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    X = (image.transpose(2,1).transpose(0,1).view([1,3,224,224]) - mean) / std\n",
    "    \n",
    "    layers = list(model._modules['features']) + toconv(list(model._modules['classifier']))\n",
    "    L = len(layers)\n",
    "    \n",
    "    \n",
    "    A = [X]+[None]*L\n",
    "    with torch.no_grad():\n",
    "      for l in range(L): A[l+1] = layers[l].forward(A[l]).to(device)\n",
    "    \n",
    "    scores = np.array(A[-1].data.view(-1).cpu())\n",
    "    ind = np.argsort(-scores)\n",
    "    \n",
    "    if verbose:\n",
    "        for i in ind[:5]:\n",
    "            print('%20s (%3d): %6.3f'%(imgclasses[i][:20],i,scores[i]))\n",
    "\n",
    "    T = torch.FloatTensor((1.0*(np.arange(1000)==class_id).reshape([1,1000,1,1]))).to(device)\n",
    "    R = [None] * L + [(A[-1]*T)]\n",
    "\n",
    "    for l in range(1,L)[::-1]:\n",
    "\n",
    "        A[l] = A[l].requires_grad_(True)\n",
    "\n",
    "        if isinstance(layers[l],torch.nn.MaxPool2d): layers[l] = torch.nn.AvgPool2d(2)\n",
    "        \n",
    "        if isinstance(layers[l],torch.nn.Conv2d) or isinstance(layers[l],torch.nn.AvgPool2d):\n",
    "\n",
    "            \n",
    "            if l <= 16:       rho = lambda p: p + 0.25*p.clamp(min=0); incr = lambda z: z+1e-9\n",
    "            if 17 <= l <= 30: rho = lambda p: p;                       incr = lambda z: z+1e-9+0.25*((z**2).mean()**.5).data\n",
    "            if l >= 31:       rho = lambda p: p;                       incr = lambda z: z+1e-9\n",
    "\n",
    "            z = incr(newlayer(layers[l],rho).forward(A[l]))                     # step 1\n",
    "            s = (R[l+1]/z).data                                                 # step 2\n",
    "            (z*s).sum().backward(); c = A[l].grad                               # step 3\n",
    "            R[l] = (A[l]*c).data                                                # step 4\n",
    "            \n",
    "        else:\n",
    "            if not dropout:\n",
    "              R[l] = R[l+1]\n",
    "            else:\n",
    "              if isinstance(layers[l],MyDropout):\n",
    "                  incr = lambda z: z+1e-9\n",
    "                  z = incr(layers[l].forward(A[l], freeze = True))\n",
    "                  s = (R[l+1]/z).data                                           # step 2\n",
    "                  (z*s).sum().backward(); c = A[l].grad                         # step 3\n",
    "                  R[l] = (A[l]*c).data                                          # step 4\n",
    "              else:\n",
    "                  R[l] = R[l+1]\n",
    "\n",
    "    A[0] = A[0].requires_grad_(True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      lb = (A[0]*0+(0-mean)/std)\n",
    "      hb = (A[0]*0+(1-mean)/std)\n",
    "    \n",
    "    lb = lb.requires_grad_(True)\n",
    "    hb = hb.requires_grad_(True)\n",
    "\n",
    "    z = layers[0].forward(A[0]) + 1e-9                                          # step 1 (a)\n",
    "    z -= newlayer(layers[0],lambda p: p.clamp(min=0)).forward(lb)               # step 1 (b)\n",
    "    z -= newlayer(layers[0],lambda p: p.clamp(max=0)).forward(hb)               # step 1 (c)\n",
    "    s = (R[1]/z).data                                                           # step 2\n",
    "    (z*s).sum().backward(); c,cp,cm = A[0].grad,lb.grad,hb.grad                 # step 3\n",
    "    #print(c,cp,cm)\n",
    "    R[0] = (A[0]*c+lb*cp+hb*cm).data\n",
    "    \n",
    "    return R[0].data\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Clone a layer and pass its parameters through the function g\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def newlayer(layer,g):\n",
    "\n",
    "    layer = copy.deepcopy(layer)\n",
    "\n",
    "    try: layer.weight = nn.Parameter(g(layer.weight))\n",
    "    except AttributeError: pass\n",
    "\n",
    "    try: layer.bias   = nn.Parameter(g(layer.bias))\n",
    "    except AttributeError: pass\n",
    "\n",
    "    return layer\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# convert VGG classifier's dense layers to convolutional layers\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def toconv(layers):\n",
    "\n",
    "    newlayers = []\n",
    "\n",
    "    for i,layer in enumerate(layers):\n",
    "\n",
    "        if isinstance(layer,nn.Linear):\n",
    "\n",
    "            newlayer = None\n",
    "\n",
    "            if i == 0:\n",
    "                m,n = 512,layer.weight.shape[0]\n",
    "                newlayer = nn.Conv2d(m,n,7)\n",
    "                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,7,7))\n",
    "\n",
    "            else:\n",
    "                m,n = layer.weight.shape[1],layer.weight.shape[0]\n",
    "                newlayer = nn.Conv2d(m,n,1)\n",
    "                newlayer.weight = nn.Parameter(layer.weight.reshape(n,m,1,1))\n",
    "\n",
    "            newlayer.bias = nn.Parameter(layer.bias)\n",
    "\n",
    "            newlayers += [newlayer]\n",
    "\n",
    "        else:\n",
    "            newlayers += [layer]\n",
    "\n",
    "    return newlayers\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Function for MinMax Normalisation of Relevances\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def normalise_relevance(relevance_matrix):\n",
    "    a = relevance_matrix.min()\n",
    "    b = relevance_matrix.max()\n",
    "    \n",
    "    if (a == 0.) & (b == 0.):\n",
    "        return relevance_matrix\n",
    "    if (a > 0.):\n",
    "      return (relevance_matrix >0.)*relevance_matrix/b\n",
    "    if (b < 0.):\n",
    "      return - (relevance_matrix <=0.)*relevance_matrix/a\n",
    "    \n",
    "    return (relevance_matrix >0.)*relevance_matrix/b  - (relevance_matrix <=0.)*relevance_matrix/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksc9gYIDx4Fv"
   },
   "source": [
    "### Visualization of B-LRP on the 'castle' example\n",
    "In a following cells we inspect the B-LRP visually on the example of 'castle' image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "i5J9V_2Xgrkp",
    "outputId": "237ca32d-9918-4f8e-c5b7-31d9b3507c54"
   },
   "outputs": [],
   "source": [
    "img_name = 'castle.jpg'\n",
    "img = np.array(cv2.imread(img_name))[...,::-1]/255.0\n",
    "plt.imshow(img)\n",
    "\n",
    "img = torch.tensor(img).float().to(device)\n",
    "class_ind = 483                                         # index corresponding to a 'castle' class in Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6Cbyxi4o6ZG1",
    "outputId": "cef3ab95-4854-40b2-ad90-acb77f23f57e"
   },
   "outputs": [],
   "source": [
    "N_MC = 100                                              # Number of samples from the posterior\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "LRPs = torch.zeros([N_MC, 224, 224])\n",
    "Standard_LRP = torch.zeros([224, 224])\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(range(N_MC)):\n",
    "  LRPs[i] = LRP_CMP(img, class_ind, model, dropout = True, device = device)[0].sum(axis = 0).data\n",
    "  LRPs[i][LRPs[i] != LRPs[i]] = 0.\n",
    "\n",
    "Standard_LRP = LRP_CMP(img, class_ind, model, dropout = False, device = device)[0].sum(axis = 0).data\n",
    "Standard_LRP[Standard_LRP != Standard_LRP] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "colab_type": "code",
    "id": "rgulzISf-zyR",
    "outputId": "509fce40-305c-4e49-9411-900bce7f9f20"
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "alphas = [5,25,50,75,95]\n",
    "\n",
    "LRPs = LRPs.to('cpu')\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15.6,3))\n",
    "\n",
    "ax[0].imshow(img.cpu(), cmap = 'gray')\n",
    "ax[0].title.set_text('Original Image')\n",
    "ax[0].xaxis.set_major_locator(plt.NullLocator())\n",
    "ax[0].yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "ax[1].imshow(normalise_relevance(Standard_LRP.cpu()), cmap = 'seismic')\n",
    "ax[1].title.set_text('Standard LRP')\n",
    "ax[1].xaxis.set_major_locator(plt.NullLocator())\n",
    "ax[1].yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "ax[2].imshow(normalise_relevance(LRPs.mean(axis  = 0)), cmap = 'seismic')\n",
    "ax[2].title.set_text('Expected LRP')\n",
    "ax[2].xaxis.set_major_locator(plt.NullLocator())\n",
    "ax[2].yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "fig, ax = plt.subplots(1, len(alphas), figsize=(14,6))\n",
    "\n",
    "cols = ['{}-th Percentile'.format(col) for col in alphas]\n",
    "\n",
    "for axe, col in zip(ax, cols):\n",
    "    axe.set_title(col)\n",
    "\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    ax[i].imshow(normalise_relevance(np.percentile(LRPs.reshape([N_MC, - 1]).numpy(), alphas[i], axis = 0).reshape([224,224])), cmap='seismic')\n",
    "    ax[i].xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax[i].yaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLli7WxNzziX"
   },
   "source": [
    "### B-LRP Pixelflipping with LRP-epsilon\n",
    "In the following we perform a pixelflipping comparison between B-LRP and LRP-epsilon rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "in0t1YDwz2C3"
   },
   "source": [
    "For the first step we need to load the subset of Imagenet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "oaflS4VsadhO",
    "outputId": "20554677-c10e-4a02-9d79-b7f168adc203"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder('imagenet_data', transform=transform)\n",
    "\n",
    "target = {0: 483,                      # Indexes, coresponding to a true classes in a subset\n",
    "         1: 562,\n",
    "         2: 951,\n",
    "         3: 355,\n",
    "         4: 721,\n",
    "         5: 932,\n",
    "         6: 849,\n",
    "         7: 282,\n",
    "         8: 980,\n",
    "         9: 907,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ii2X47EEbCXl"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# LRP Epsilon rule\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# More information at http://www.heatmapping.org/tutorial/\n",
    "\n",
    "def LRP_epsilon(image, class_id, model, dropout = True, verbose = False, device = 'cpu', epsilon = 1e-9):\n",
    "\n",
    "    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(1,-1,1,1).to(device)  # We define mean and std for data normalisation\n",
    "    std  = torch.Tensor([0.229, 0.224, 0.225]).reshape(1,-1,1,1).to(device)\n",
    "    image = image.to(device)\n",
    "   \n",
    "    if not dropout:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    X = (image.view([1,3,224,224]).to(device) - mean) / std\n",
    "    \n",
    "    layers = list(model._modules['features']) + toconv(list(model._modules['classifier']))\n",
    "    L = len(layers)\n",
    "\n",
    "    A = [X]+[None]*L\n",
    "    with torch.no_grad():\n",
    "      for l in range(L): A[l+1] = layers[l].forward(A[l]).to(device)\n",
    "    \n",
    "    scores = np.array(A[-1].data.view(-1).cpu())\n",
    "    ind = np.argsort(-scores)\n",
    "    \n",
    "    if verbose:\n",
    "        for i in ind[:3]:\n",
    "            print('New instance:')\n",
    "            print('%20s (%3d): %6.3f'%(imgclasses[i][:20],i,scores[i]))\n",
    "\n",
    "    T = torch.FloatTensor((1.0*(np.arange(1000)==class_id).reshape([1,1000,1,1]))).to(device)\n",
    "\n",
    "    R = [None] * L + [(A[-1]*T)]\n",
    "\n",
    "    for l in range(0,L)[::-1]:\n",
    "\n",
    "        A[l] = A[l].requires_grad_(True)\n",
    "        rho = lambda p: p;                       incr = lambda z: z + epsilon\n",
    "\n",
    "        if isinstance(layers[l],torch.nn.MaxPool2d): layers[l] = torch.nn.AvgPool2d(2)\n",
    "        \n",
    "        if isinstance(layers[l],torch.nn.Conv2d) or isinstance(layers[l],torch.nn.AvgPool2d):\n",
    "\n",
    "            z = incr(newlayer(layers[l],rho).forward(A[l]))              # step 1\n",
    "            s = (R[l+1]/z).data                                          # step 2\n",
    "            (z*s).sum().backward(); c = A[l].grad                        # step 3\n",
    "            R[l] = (A[l]*c).data                                         # step 4\n",
    "            \n",
    "        else:\n",
    "            if not dropout:\n",
    "              R[l] = R[l+1]\n",
    "            else:\n",
    "              if isinstance(layers[l],MyDropout):\n",
    "                  incr = lambda z: z + epsilon\n",
    "                  z = incr(layers[l].forward(A[l], freeze = True))       # step 1\n",
    "                  s = (R[l+1]/z).data                                    # step 2\n",
    "                  (z*s).sum().backward(); c = A[l].grad                  # step 3\n",
    "                  R[l] = (A[l]*c).data                                   # step 4\n",
    "              else:\n",
    "                  R[l] = R[l+1]\n",
    "    return R[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rEDlWOhb18uI"
   },
   "source": [
    "In the following cell we define a function for pixelflipping\n",
    "\n",
    "To speed up the computation, we evaluate model's output scores not each time we flip 1 pixel, but when we flip *steps* pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LmF40CU1usB"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Function for performing a pixelflipping\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def pixelflipping(image, class_ind, R, model, inner_steps, step_size):\n",
    "\n",
    "  # image:        Original Image,\n",
    "  # class_ind:    Index of a true class,\n",
    "  # R:            Relevances,\n",
    "  # model:        Neural Network,\n",
    "  # inner_steps:  Number of times we evaluate scores on the augmented image,\n",
    "  # step_size:    Number of pixels flipped between the evaluations,\n",
    "\n",
    "  mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(1,-1,1,1).to(device) \n",
    "  std  = torch.Tensor([0.229, 0.224, 0.225]).reshape(1,-1,1,1).to(device)\n",
    "  \n",
    "  img = image.clone()\n",
    "  relevances = R.clone()\n",
    "  scores = torch.zeros(inner_steps)\n",
    "  model.eval()\n",
    "\n",
    "  for i in range(inner_steps):\n",
    "    scores[i] = model(img.to(device).view([1,3,224,224]))[0][class_ind].data\n",
    "    \n",
    "    for s in range(step_size):\n",
    "        ind = np.unravel_index(torch.argmax(relevances).cpu(), relevances.shape)\n",
    "        img[0][ind[0]][ind[1]] = (np.random.uniform(0,1,1)[0] - mean.view(3)[0])/std.view(3)[0]\n",
    "        img[1][ind[0]][ind[1]] = (np.random.uniform(0,1,1)[0] - mean.view(3)[1])/std.view(3)[1]\n",
    "        img[2][ind[0]][ind[1]] = (np.random.uniform(0,1,1)[0] - mean.view(3)[2])/std.view(3)[2]\n",
    "\n",
    "        relevances[ind] = -np.Inf\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odorZStkbIZj"
   },
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "\n",
    "N_pics = 1                        # Number of images used in pixelflipping, (in main experiment 300 was used)\n",
    "N_inner = 128                     # Number of times we evaluate scores on the augmented image,\n",
    "N_MC = 100                        # Number of times we sample the posterior to get an estimation for percentiles in B-LRP\n",
    "steps = 66                        # Number of pixels flipped between the evaluations,\n",
    "alphas = [5, 25, 50, 75, 95]      # Alphas used in B-LRP\n",
    "\n",
    "\n",
    "score_lrp = torch.zeros([N_pics, N_inner]).data.to(device)\n",
    "score_random= torch.zeros([N_pics, N_inner]).data.to(device)\n",
    "score_alphas= torch.zeros([len(alphas), N_pics, N_inner]).data.to(device)\n",
    "\n",
    "samples = np.random.choice(len(testset), N_pics)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "counter = 0\n",
    "for q in tqdm(samples):\n",
    "  \n",
    "  img = testset[q][0].to(device)\n",
    "\n",
    "  class_ind = target[testset[q][1]]\n",
    "\n",
    "  LRP_MAP = LRP_epsilon(img, class_ind, model, dropout = False, verbose = False, device = device)[0].sum(axis = 0).data\n",
    "\n",
    "  LRPs = torch.zeros([N_MC, 224, 224])\n",
    "  for i in range(N_MC):\n",
    "    LRPs[i] = LRP_epsilon(img, class_ind, model, dropout = True, verbose = False, device = device)[0].sum(axis = 0).data\n",
    "\n",
    "  LRP_ALPHAs = torch.zeros([len(alphas), 224,224])\n",
    "  \n",
    "  for i in range(len(alphas)):\n",
    "    LRP_ALPHAs[i] = torch.tensor(np.percentile(LRPs.reshape([N_MC, - 1]).numpy(), alphas[i], axis = 0).reshape([224,224]))\n",
    "\n",
    "  score_lrp[counter]= pixelflipping(img, class_ind, LRP_MAP, model, N_inner, steps)\n",
    "\n",
    "  for i in range(len(alphas)):\n",
    "    score_alphas[i][counter] = pixelflipping(img, class_ind, LRP_ALPHAs[i], model, N_inner, steps)\n",
    "\n",
    "\n",
    "  counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNpIJYTNbhBl"
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "fig, ax = plt.subplots(figsize=(8,6), num='seaborn')\n",
    "\n",
    "ax.xaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "x =  np.linspace(0, 100*(N_inner*steps/224/224), N_inner )\n",
    "ax.grid(True, which=\"both\", ls=\"-\")\n",
    "ax.set_ylabel('Mean output score')\n",
    "ax.set_xlabel('Percentage of pixels flipped')\n",
    "ax.set_title('Imagenet')\n",
    "\n",
    "ax.plot(x, score_lrp[:300].cpu().mean(axis = 0).detach().numpy(), label='Standard LRP',linewidth=2,markersize=10, marker = '*')\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    ax.plot(x, score_alphas[i][:300].cpu().mean(axis = 0).detach().numpy()\n",
    "    , label=r'B-LRP $\\alpha = $' + str(alphas[i]),linewidth=1.5,markersize=4, marker = '.')\n",
    "\n",
    "ax.legend()\n",
    "plt.savefig(\"IMAGENET_PIXELFLIPPING_FINAL.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "B-LRP: Imagenet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
